# Yin, Zhu and Hu, 2022

* New privacy concerns have emerged during the aggregation of distributed intermediate results. 
* Privacy-preserving federated learning (PPFL) has been heralded as a solution to generic privacy-preserving machine learning.
* Maintain data utility while protecting data privacy.
* Prompt a 5W-scenario-based taxonomy.
* Big data pushes the development of FL.

## Challenges and Rooms

* A method of effectively apply privacy-preserving mechanisms should be determined.
* The intermediate weights or gradients between clients and server will reveal sensitive information about training sets. Two ways mentioned are encrypting the communication channel from eavesdropping, such as homomorphic encryption, secret sharing, and multi-party secure computation; or protect the weight and gradient updates by perturbation, though the noise will simultaneously degrade the model accuracy and cause computational overhead.
* Final model can be used to extracted sensitive information as well if not protected properly.
* Data memorisation should be handled in the PPFL to prevent privacy leakage - neural network models might sometimes memorise sensitive information of training data.
* Privacy-preserving mechanisms may differ in terms of the effectiveness and the computation cost, when FL is applied for privacy defence.
